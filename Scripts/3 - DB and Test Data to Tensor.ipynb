{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model with the Trained Bi-Encoder for encoding addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('~/Bi-Encoder Fine-Tuned', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '~/data'\n",
    "test_data = pd.read_csv(data_folder + \"test_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code defines a class called ``UnNormalizedDataset`` which inherits from ``torch.utils.data.Dataset``. The purpose of this class is to create a custom dataset that can be used to load data into a PyTorch model.\n",
    "\n",
    "The __init__ method of the ``UnNormalizedDataset`` class takes a parameter ``data`` which represents the data to be loaded into the model. This data is stored as an attribute of the class instance called ``self.data``.\n",
    "\n",
    "The __len__ method of the ``UnNormalizedDataset`` class returns the length of the data, which is the number of samples in the dataset.\n",
    "\n",
    "The __getitem__ method of the ``UnNormalizedDataset`` class is used to retrieve a single sample from the dataset. It takes an index ``idx`` as a parameter, which represents the index of the sample to be retrieved. The method retrieves the \"UnnormalizedAddress\" value of the sample at the given index from the ``self.data`` attribute and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        morada_un_normalized = self.data.iloc[idx]['UnnormalizedAddress']\n",
    "        return morada_un_normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Test Data with the ``UnnormalizedDataset`` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UnNormalizedDataset(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ``Dataloader`` to make the batches from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the Test Dataset. <br>\n",
    "Using batches as big as possible to faster encoding. <br>\n",
    "Convert to tensor and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_normalized_encoded = torch.Tensor().to(device=device)\n",
    "\n",
    "for x in tqdm(iter(dataset_dataloader)):  \n",
    "    encoded_batch = model.encode(x, batch_size=128, device=device, convert_to_tensor = True)\n",
    "    un_normalized_encoded = torch.cat((un_normalized_encoded, encoded_batch), 0)\n",
    "    \n",
    "\n",
    "torch.save(un_normalized_encoded, '~/test_data_embeddings.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Normalized Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '~/data'\n",
    "normalized_data = pd.read_csv(data_folder + \"normalized_database.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code defines a class called ``NormalizedDataset`` which inherits from ``torch.utils.data.Dataset``. The purpose of this class is to create a custom dataset that can be used to load data into a PyTorch model.\n",
    "\n",
    "The __init__ method of the ``NormalizedDataset`` class takes a parameter ``data`` which represents the data to be loaded into the model. This data is stored as an attribute of the class instance called ``self.data``.\n",
    "\n",
    "The __len__ method of the ``NormalizedDataset`` class returns the length of the data, which is the number of samples in the dataset.\n",
    "\n",
    "The __getitem__ method of the ``NormalizedDataset`` class is used to retrieve a single sample from the dataset. It takes an index ``idx`` as a parameter, which represents the index of the sample to be retrieved. The method retrieves the \"NormalizedAddress\" value of the sample at the given index from the ``self.data`` attribute and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        morada_normalized = self.data.iloc[idx]['NormalizedAddress']\n",
    "        return morada_normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Normalized Data with the ``NormalizedDataset`` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset = NormalizedDataset(normalized_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ``Dataloader`` to make the batches from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataloader = DataLoader(dataset=normalized_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the Normalized Data. <br>\n",
    "Using batches as big as possible to faster encoding. <br>\n",
    "Convert to tensor and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_encoded = torch.Tensor().to(device=device)\n",
    "\n",
    "for x in tqdm(iter(dataset_dataloader)):  \n",
    "    encoded_batch = model.encode(x, batch_size=128, device=device, convert_to_tensor = True)\n",
    "    normalized_encoded = torch.cat((normalized_encoded, encoded_batch), 0)\n",
    "    \n",
    "\n",
    "torch.save(normalized_encoded, '~/normalized_data_embeddings.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
