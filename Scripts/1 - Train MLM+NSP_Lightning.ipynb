{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, Trainer, BertForPreTraining, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "import random\n",
    "\n",
    "\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed_everything(42, workers=True) function call sets the seed for the random number generators used in the experiment to the integer value of 42. <br>\n",
    "Setting the random seed to a fixed value can help to ensure that the results of the experiment are reproducible, meaning that they can be repeated with the same results. <br>\n",
    "The workers=True argument specifies that the random seed should also be set for any worker processes that may be used in parallel processing, which can further improve reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"TrainBERTscratchMLMandNSP\"\n",
    "run_rame = \"run_01\"\n",
    "wandb.init(project= project,\n",
    "                 config={\n",
    "                    \"csv_path\": '~/data/MLM+NSP_BERT_Data_for_Dataset.csv',\n",
    "                    \"batch_size\": 16,\n",
    "                    \"max_epochs\": 100,                    \n",
    "                    \"validation_size\": 2000,\n",
    "                    \"shuffle_train\": True,\n",
    "                    \"optimizer\": AdamW,\n",
    "                    \"learning_rate\": 1e-4,\n",
    "                    \"weight_decay\": 0.01,\n",
    "                    \"warmup_function\": 'Linear',\n",
    "                    \"warmup_steps\": 0.1,\n",
    "                    \"early_stop\": False,\n",
    "                    \"patience\": 10,\n",
    "                    \"run_name\": run_rame,\n",
    "                    \"model_name\": '~/models/BERT after MLM+NSP',\n",
    "                    \"model_save_path\": '~/models/BERT after MLM+NSP/' + run_rame\n",
    "                 })\n",
    "\n",
    "config_wandb = wandb.config\n",
    "wandb_logger = WandbLogger(name=config_wandb.run_name, project=project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>\n",
    "This code block creates an instance of the `BertConfig` class from the `transformers` library. `BertConfig` is a configuration class that is used to define the architecture and parameters of a BERT (Bidirectional Encoder Representations from Transformers) model. \n",
    "</p>\n",
    "<p>\n",
    "The constructor of `BertConfig` takes several arguments, each of which corresponds to a different aspect of the model's architecture or hyperparameters. In this case, the following arguments are specified:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>`vocab_size`: This sets the size of the model's vocabulary. Here, it is set to 30,000.</li>\n",
    "    <li>`hidden_size`: This sets the number of hidden units in each layer of the model. Here, it is set to 768.</li>\n",
    "    <li>`num_hidden_layers`: This sets the number of layers in the model. Here, it is set to 6.</li>\n",
    "    <li>`num_attention_heads`: This sets the number of attention heads used in the multi-head attention mechanism of the model. Here, it is set to 12.</li>\n",
    "    <li>`max_position_embeddings`: This sets the maximum length of the input sequences that the model can handle. Here, it is set to 512.</li>\n",
    "</ul>\n",
    "<p>\n",
    "By creating an instance of `BertConfig` with these values, the model architecture and hyperparameters are defined, and can be used to initialize a BERT model for natural language processing tasks.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=6, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>\n",
    "This code block initializes a BERT model for pretraining, using a custom tokenizer and a specified device (either CPU or GPU, depending on availability).\n",
    "</p>\n",
    "<p>\n",
    "The first line sets the device to be used for training and inference. If a GPU is available, it is set to \"cuda\"; otherwise, it defaults to \"cpu\".\n",
    "</p>\n",
    "<p>\n",
    "The second line initializes a tokenizer object using the `AutoTokenizer` class from the `transformers` library. The `from_pretrained()` method is used to load a custom tokenizer located at the specified path ('~/Custom_Tokenizer'), and the `use_fast=True` argument enables the use of a fast tokenizer implementation for improved performance.\n",
    "</p>\n",
    "<p>\n",
    "Finally, the third line initializes a `BertForPreTraining` model object using the `config` object created in the previous code block, and sends it to the specified device using the `to()` method. `BertForPreTraining` is a variant of the BERT model that is designed for pretraining, and includes additional prediction tasks to improve the model's ability to learn general representations of language.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('~/Custom_Tokenizer', use_fast=True)\n",
    "bert_model = BertForPreTraining(config).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will assign a new Porta_ID value to each address. It is used to be able to do data[idx] when creating the dataset class. The reason why I have to do this every time I run the program is that I split the data randomly into training and testing sets. Only after this division can I calculate the new ID for each address, because if I do it beforehand, the division of IDs between training and testing will be random and doing data[idx] later will be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_ID(data):\n",
    "    new_door_id = []\n",
    "    aux = 0\n",
    "    for i in tqdm(range(len(data))):\n",
    "        if(i == 0):\n",
    "            new_door_id.append(0)\n",
    "        else:\n",
    "            door_id_atual = data.at[i, 'Porta_ENT']\n",
    "            door_id_antigo = data.at[i-1, 'Porta_ENT']\n",
    "            if(door_id_antigo != door_id_atual):\n",
    "                aux = aux+1\n",
    "                new_door_id.append(aux)\n",
    "            else:\n",
    "                new_door_id.append(aux)\n",
    "    data['ID'] = new_door_id\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I perform the split, it may happen that addresses that are only written in two ways (normalized and unnormalized) get separated. This is a problem because then I can't pair them in the dataloader batches. This cell will ensure that addresses are grouped by their ID value, and therefore, in the split, addresses that share the same Porta_ENT are placed in the same group. I will try to maintain an approximate ratio of 90% training and 10% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data, train_size_proportion):\n",
    "    # group the dataframe by the non-unique column\n",
    "    groups = data.groupby('Porta_ENT')\n",
    "\n",
    "    # randomly assign each group to either split 1 or split 2\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "\n",
    "\n",
    "    group_ids = np.random.choice([0, 1], size=len(groups), p=[train_size_proportion, 0.1])\n",
    "    group_dict = dict(zip(groups.groups.keys(), group_ids))\n",
    "\n",
    "    # create the two splits\n",
    "    train = pd.concat([group for key, group in groups if group_dict[key] == 0]).sort_values(by=['Porta_ENT']).reset_index(drop=True)\n",
    "    test = pd.concat([group for key, group in groups if group_dict[key] == 1]).sort_values(by=['Porta_ENT']).reset_index(drop=True)\n",
    "\n",
    "    train = add_new_ID(train)\n",
    "    test = add_new_ID(test)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to create a dataset that, when called to create batches, for each index that I request, returns a pair of addresses. Each address can be written in at least 2 ways: normalized + unnormalized. Therefore, when I do dataset[idx], it is always possible to return something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['ID'].nunique() #Number of unique IDs.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df_aux = self.data[self.data['ID'] == idx].sample(n=2, replace=False).reset_index(drop=True)\n",
    "        sa = df_aux.iloc[0]['Address']\n",
    "        sb = df_aux.iloc[1]['Address']\n",
    "        label = 0\n",
    "        return {'Address_1': sa, 'Address_2': sb, 'Label': label}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function: collate_MLM_NSP(batch)</h3>\n",
    "<p>This function takes in a batch of examples, where each example is a set containing two addresses and a label. It then performs operations to tokenize the addresses, create negative samples for the next sentence prediction (NSP) task, and create a masked language model (MLM) mask. Finally, it returns a dictionary containing the tensors that will go into the model.</p>\n",
    "<p><strong>Input:</strong> a batch of examples, where each example is a set containing two addresses and a label.</p>\n",
    "<p ><strong>Output:</strong> a dictionary containing the tensors that will go into the model.</p>\n",
    "<h4>Steps:</h4>\n",
    "<ol>\n",
    "    <li>Create empty lists for address_1, address_2, and next_sentence_label</li>\n",
    "    <li>Loop through the examples in the batch, and append the address_1, address_2, and next_sentence_label of each example to their respective lists</li>\n",
    "    <li>Convert the batch labels for NSP task to tensor with dtype long and shape (batch_size,)</li>\n",
    "    <li>Assign a uniformly generated probability value between [0-1] to each element of the batch. This will determine which addresses will be swapped to create negative pairs for the NSP task.</li>\n",
    "    <li>Mark cases where the probability was >0.5 as cases where a swap will occur</li>\n",
    "    <li>Find indices where [mask] == True</li>\n",
    "    <li>Create a copy of the list that contains the addresses that will be swapped. This is to avoid swapping a vector with itself.</li>\n",
    "    <li>Loop through the indices where a swap will occur, and choose an address for the swap</li>\n",
    "    <li>Place the new addresses, in the correct positions in the initial list</li>\n",
    "    <li>Swap labels in positions where there was a swap</li>\n",
    "    <li>Tokenize address pairs using the tokenizer</li>\n",
    "    <li>Create the MLM mask by looping through the tokenized input_ids, and assigning a mask token with a probability of 15% to valid tokens. Valid tokens are those that are not [CLS, PAD or SEP].</li>\n",
    "    <li>Change the labels tensor in order to fill with -100 all the entries where there are no [MASK] tokens.</li>\n",
    "    <li>Create a dictionary containing the tensors that will go into the model, and send them to the GPU. Pytorch Lightning will not do this by default in this case.</li>\n",
    "    <li>Return the dictionary of inputs.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_MLM_NSP(batch):\n",
    "    address_1 = []\n",
    "    address_2 = []\n",
    "    next_sentence_label  = []\n",
    "    \n",
    "    # I have to receive each example from the batch individually. It receives sets [{'Address_1': sa, 'Address_2': sb, 'Label': label}] of these.\n",
    "    # Therefore, to perform operations, I have to create lists again for each of the elements.\n",
    "    for example in batch:\n",
    "        address_1.append(example['Address_1'])\n",
    "        address_2.append(example['Address_2'])\n",
    "        next_sentence_label .append(example['Label'])\n",
    "\n",
    "    # Convert batch labels for NSP task to tensor. dtype should be long. shape(batch_size,)\n",
    "    next_sentence_label  = torch.tensor(next_sentence_label , dtype=torch.long)\n",
    "\n",
    "\n",
    "    # Assign a uniformly generated probability value between [0-1] to each element of the batch. I will find out which addresses will be swapped\n",
    "    # to create negative pairs for the NSP task.\n",
    "    probabilities = np.random.uniform(0, 1, len(address_2))\n",
    "\n",
    "    # Cases where probability was >0.5 will be marked as cases where a swap will occur\n",
    "    mask = probabilities > 0.5\n",
    "\n",
    "\n",
    "    # Find indices where [mask] == True.\n",
    "    indices = np.arange(len(address_2))[mask]\n",
    "    \n",
    "\n",
    "    # Here I create a copy of the list that contains the addresses that I am going to swap to make the changes I want. If I use only the original one, an undesired phenomenon\n",
    "    # could happen. Ex: Indices that will swap [1, 5, 7]. Swapping with [7, 2, 1]. Using only one vector, position 7 will end up with the same vector it started with,\n",
    "    # as it was initially swapped with position 1. I have to make sure that a vector does not swap with itself.\n",
    "    lst = np.array(address_2)\n",
    "    lst_aux = np.array(address_2)\n",
    "\n",
    "    for i in indices:\n",
    "        # Choose an address for the swap\n",
    "        lst_aux[i] = lst[int(np.random.choice(np.delete(np.arange(len(lst)), i), size=1))]\n",
    "    \n",
    "    # Place the new addresses, in the correct positions in the initial list\n",
    "    lst[mask] = lst_aux[mask]\n",
    "\n",
    "    # Swap labels in positions where there was a swap\n",
    "    next_sentence_label [mask] = 1\n",
    "\n",
    "    # Tokenize address pairs\n",
    "    tokenized = tokenizer(address_1, lst.tolist(), return_tensors='pt',  add_special_tokens=True, padding='longest')\n",
    "\n",
    "    # Label matrix (batch_size x sequence length) initially receives the input_ids after encoding. Then I will do masking and update this tensor\n",
    "    # to count only valid values in positions where there is a [MASK] token. The detach().clone() is done to ensure that changes made to tokenized.input_ids\n",
    "    # do not reflect in the labels tensor.\n",
    "    labels = tokenized.input_ids.detach().clone()\n",
    "\n",
    "\n",
    "    # Create the MLM mask\n",
    "    prob_mask = 0.15\n",
    "    mlm_mask = []\n",
    "\n",
    "    # Loop to assign mask tokens with a probability of 15% to tokenized.input_ids. I make sure not to give [MASK] to [CLS, PAD or SEP].\n",
    "    for i in range(tokenized.input_ids.shape[0]):\n",
    "        for j in range(tokenized.input_ids.shape[1]):\n",
    "            if random.uniform(0, 1) < prob_mask and tokenized.input_ids[i][j] not in [tokenizer.sep_token_id, tokenizer.cls_token_id, tokenizer.pad_token_id]:\n",
    "                mlm_mask.append(j)\n",
    "        mlm_mask = []\n",
    "\n",
    "    # Change labels tensor in order to fill with -100 all the entries where there are no [MASK] tokens.\n",
    "    labels = torch.where(tokenized.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "    # Create dictionary where I send to GPU all the tensors that will go to the model. Pytorch Lightning will not do this by default in this case.\n",
    "    inputs = {'input_ids': tokenized.input_ids.to(device),\n",
    "              'token_type_ids': tokenized.token_type_ids.to(device),\n",
    "              'attention_mask': tokenized.attention_mask.to(device),\n",
    "              'labels': labels.to(device),\n",
    "              'next_sentence_label': next_sentence_label.to(device)}\n",
    "    \n",
    "\n",
    "    return inputs\n",
    "               \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>This is a Python class called <code>MatchModel</code> that extends the <code>LightningModule</code> class. It defines a neural network model and its training and validation steps.</p>\n",
    "<h3>Constructor</h3>\n",
    "<p>The constructor initializes various instance variables including the model, csv_path, batch_size, epochs, warmup_steps, best_val_loss, and softmax function.</p>\n",
    "<h3>Setup</h3>\n",
    "<p>The <code>setup</code> method is called when the class is initialized. It reads a CSV file from the path specified in <code>csv_path</code> and splits the data into train and test sets. It then initializes two instances of the <code>MatchDataset</code> class for the train and test sets.</p>\n",
    "<h3>Data Loaders</h3>\n",
    "<p>The <code>train_dataloader</code> and <code>val_dataloader</code> methods return <code>DataLoader</code> instances for the train and validation datasets, respectively. They use the <code>collate_MLM_NSP</code> function to collate batches.</p>\n",
    "<h3>Forward Method</h3>\n",
    "<p>The <code>forward</code> method takes an input dictionary (the return of the collate_fn of the dataloader) and passes it to the <code>model</code> instance defined in the constructor.</p>\n",
    "<h3>Training Step</h3>\n",
    "<p>The <code>training_step</code> method is called for each batch during the training phase. It calls the <code>forward</code> method to obtain model predictions and calculates the loss. The <code>log</code> function is used to log the loss value.</p>\n",
    "<h3>Validation Step</h3>\n",
    "<p>The <code>validation_step</code> method is called for each batch during the validation phase. It calls the <code>forward</code> method to obtain model predictions and calculates the loss, as well as the accuracy for both the MLM and NSP tasks.</p>\n",
    "<h3>Validation Epoch End</h3>\n",
    "<p>The <code>validation_epoch_end</code> method is called at the end of each validation epoch. It calculates the average validation loss and accuracies over all validation batches and logs them. If the current epoch loss is lower than the best loss obtained so far, the model is saved to disk.</p>\n",
    "<h3>Configure Optimizers</h3>\n",
    "<p>The <code>configure_optimizers</code> method initializes the optimizer and learning rate scheduler for the model.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchModel(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.csv_path = config_wandb.csv_path\n",
    "        self.batch_size = config_wandb.batch_size\n",
    "        self.epochs = config_wandb.max_epochs\n",
    "        self.warmup_steps = config_wandb.warmup_steps\n",
    "\n",
    "    \n",
    "        self.best_val_loss = 999 # Init with big value so that 1st epoch is always saved\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        train, test = get_train_test_data(data = df, train_size_proportion=0.9)\n",
    "        self.train_dataset = MatchDataset(train)\n",
    "        self.val_dataset = MatchDataset(test)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_MLM_NSP)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_MLM_NSP)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(**input)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        model_predictions = self.forward(batch)\n",
    "        loss = model_predictions.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        model_predictions = self.forward(batch)\n",
    "\n",
    "        #This is a combination of the MLM and the NSP Loss. (MLM: Cross-Entropy, NSP: Binary Cross-Entropy)\n",
    "        loss = model_predictions.loss\n",
    "\n",
    "        #NSP Accuracy\n",
    "        output_after_softmax_nsp = self.softmax(model_predictions['seq_relationship_logits'])\n",
    "        output_after_softmax_nsp = torch.argmax(output_after_softmax_nsp, dim=-1)        \n",
    "        acc_batch_NSP = torch.sum(output_after_softmax_nsp == batch['next_sentence_label']) / len(batch['next_sentence_label'])\n",
    "\n",
    "        #MLM Accuracy\n",
    "        valid_entries_batch = 0\n",
    "        acc_batch_MLM = 0\n",
    "        for i in range(len(batch['input_ids'])):\n",
    "            masked_indices = torch.where(batch['input_ids'][i] == tokenizer.mask_token_id)\n",
    "            valid_entries_batch = valid_entries_batch + len(masked_indices[0])\n",
    "            if(len(masked_indices[0]) > 0):\n",
    "                predictions = model_predictions['prediction_logits'][i].argmax(dim=1)\n",
    "                predicted = predictions[masked_indices]\n",
    "                true_token = batch['labels'][i][masked_indices]\n",
    "                acc_batch_MLM = acc_batch_MLM + torch.sum(predicted == true_token)\n",
    "        acc_batch_MLM = acc_batch_MLM / valid_entries_batch\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc_NSP': acc_batch_NSP, 'val_acc_MLM': acc_batch_MLM} #Isto na realidade não é 1 valor, vão ser tantos valores quantos batches tiver o validation_step\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        # Get the average values of the calculated metrics for all the validation batches.\n",
    "        current_epoch_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        current_epoch_val_acc_NSP =  torch.stack([x['val_acc_NSP'] for x in outputs]).mean()\n",
    "        current_epoch_val_acc_MLM =  torch.stack([x['val_acc_MLM'] for x in outputs]).mean()\n",
    "\n",
    "        # Log the metrics\n",
    "        self.log('val_loss', current_epoch_val_loss, prog_bar=True)\n",
    "        self.log('val_acc_NSP', current_epoch_val_acc_NSP, prog_bar=True)\n",
    "        self.log('val_acc_MLM', current_epoch_val_acc_MLM, prog_bar=True)\n",
    "\n",
    "        # Check if model should be saved by comparing the current loss with the best loss obtained so far\n",
    "        print('\\nChecking if model is to be saved: ')\n",
    "        print('Current Epoch Loss = {}, and best Loss so far = {}'.format(current_epoch_val_loss, self.best_val_loss))\n",
    "        if current_epoch_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_epoch_val_loss\n",
    "            print('Model Saved')\n",
    "            self.model.save_pretrained(config_wandb.model_save_path)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer_config = {\n",
    "            'params' : self.model.parameters(),\n",
    "            'lr' : config_wandb.learning_rate,\n",
    "            'weight_decay' : config_wandb.weight_decay,\n",
    "        }\n",
    "        optimizer = AdamW(**optimizer_config)\n",
    "        total_steps=self.trainer.estimated_stepping_batches\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= config_wandb.warmup_steps * total_steps, num_training_steps= total_steps)\n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first line creates an instance of the <code>LearningRateMonitor</code> class and assigns it to the variable <code>lr_monitor</code>. This callback monitors the learning rate of the optimizer and logs it at a specified interval during training.</p>\n",
    "<p>The second line creates an instance of the <code>EarlyStopping</code> class and assigns it to the variable <code>early_stopping_callback</code>. This callback monitors the validation loss and stops training early if the loss does not improve for a certain number of epochs specified by the <code>patience</code> argument. The <code>monitor</code> argument specifies which metric to monitor, in this case, the validation loss. The <code>mode</code> argument specifies whether to minimize or maximize the monitored metric, in this case, we want to minimize the validation loss.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=config_wandb.patience, mode='min')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>The code initializes a <code>MatchModel</code> with a <code>bert_model</code> and a <code>Trainer</code> with various arguments:</p>\n",
    "<ul>\n",
    "<li><code>accelerator</code> is set to 'gpu'</li>\n",
    "<li><code>devices</code> is set to 1</li>\n",
    "<li><code>callbacks</code> is set to a list containing <code>lr_monitor</code> and <code>early_stopping_callback</code> if <code>config_wandb.early_stop</code> is <code>True</code>, otherwise only <code>lr_monitor</code> is included</li>\n",
    "<li><code>check_val_every_n_epoch</code> is set to 1</li>\n",
    "<li><code>max_epochs</code> is set to <code>config_wandb.max_epochs</code></li>\n",
    "<li><code>enable_checkpointing</code> is set to <code>False</code></li>\n",
    "<li><code>accumulate_grad_batches</code> is set to 1</li>\n",
    "<li><code>logger</code> is set to <code>wandb_logger</code></li>\n",
    "<li><code>log_every_n_steps</code> is set to 1</li>\n",
    "<li><code>deterministic</code> is set to <code>True</code></li>\n",
    "<li><code>precision</code> is set to 16</li>\n",
    "</ul>\n",
    "<p>Finally, the <code>trainer</code> is used to fit the <code>model</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchModel(bert_model)\n",
    "trainer = Trainer(\n",
    "                    accelerator='gpu',\n",
    "                    devices=1,\n",
    "                    callbacks = [lr_monitor, early_stopping_callback] if config_wandb.early_stop == True else [lr_monitor],\n",
    "                    check_val_every_n_epoch=1,\n",
    "                    max_epochs=config_wandb.max_epochs,\n",
    "                    enable_checkpointing=False,\n",
    "                    accumulate_grad_batches=1,\n",
    "                    logger = wandb_logger,\n",
    "                    log_every_n_steps=1,\n",
    "                    deterministic=True,\n",
    "                    precision = 16,\n",
    "                    )\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.alert(\n",
    "    title=f\"Finish {config.run_name}\", \n",
    "    text = f\"Run is over.\",\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
