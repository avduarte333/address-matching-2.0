{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, models, util\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"Bi Encoder Fine Tuning with Pytorch Lightning\"\n",
    "run_rame = \"run_01\"\n",
    "wandb.init(project= project,\n",
    "                 config={\n",
    "                    \"csv_path\": '~/train_for_fine_tune BI.csv',\n",
    "                    \"validation_size\": 2000,\n",
    "                    \"shuffle_train\": True,\n",
    "                    \"optimizer\": AdamW,\n",
    "                    \"learning_rate\": 1.5e-5,\n",
    "                    \"weight_decay\": 0.01,\n",
    "                    \"warmup_function\": 'Linear',\n",
    "                    \"warmup_steps\": 0.1,\n",
    "                    \"batch_size\": 512,\n",
    "                    \"max_epochs\": 100,\n",
    "                    \"early_stop\": False,\n",
    "                    \"patience\": 10,\n",
    "                    \"run_name\": run_rame,\n",
    "                    \"model_name\": '~/models/BERT after MLM+NSP',\n",
    "                    \"model_save_path\": '~/models/' + run_rame\n",
    "                 })\n",
    "\n",
    "config = wandb.config\n",
    "wandb_logger = WandbLogger(name=config.run_name, project=project)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The code defines a class named <code>MatchDataset</code> which is a subclass of <code>torch.utils.data.Dataset</code>.<br>\n",
    "The constructor initializes an instance variable <code>data</code> which is a <code>DataFrame</code> of training data.<br>\n",
    " The <code>__len__()</code> method returns the length of the dataset, and the <code>__getitem__()</code> method takes an index and returns a single training example as an instance of the <code>InputExample</code> class.<br>\n",
    " This example contains two texts, a normalized address and an unnormalized address, along with a binary label indicating whether the two addresses match.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sa = self.data.iloc[idx]['NormalizedAddress']\n",
    "        sb = self.data.iloc[idx]['UnnormalizedAddress']\n",
    "        match = 1\n",
    "        example = InputExample(texts=[sa, sb], label=match)\n",
    "        return example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code defines a PyTorch Lightning module called <code>MatchModel</code>. The constructor of the module takes in several arguments including a <code>bi_encoder</code> object, <code>csv_path</code>, <code>batch_size</code>, <code>num_epochs</code>, and <code>num_warmup_steps</code>.<br>\n",
    "\n",
    "The <code>setup</code> method reads a CSV file from <code>csv_path</code> and creates training and validation datasets from it using <code>MatchDataset</code>.<br>\n",
    "\n",
    "The module defines two data loaders, one for training data and another for validation data using <code>DataLoader</code> and the collate function <code>self.bi_encoder.smart_batching_collate</code>.<br>\n",
    "\n",
    "The <code>forward</code> method passes the input through the <code>bi_encoder</code>.<br>\n",
    "\n",
    "The <code>training_step</code> and <code>validation_step</code> methods perform a forward pass of the input through the <code>bi_encoder</code>, calculate the cosine similarity matrix between all the embeddings (unnormalized with normalized ones) and compute the multiple negatives ranking loss.<br>\n",
    "\n",
    "The <code>validation_epoch_end</code> method calculates the average validation loss across all the batches in the validation set and logs it. It also checks whether the current validation loss is better than the best validation loss so far and saves the model if it is.<br>\n",
    "\n",
    "The <code>configure_optimizers</code> method defines an optimizer <code>(AdamW)</code> and a learning rate scheduler (<code>get_linear_schedule_with_warmup</code> or <code>CosineAnnealingLR</code>). The learning rate scheduler is used to adjust the learning rate during training.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchModel(LightningModule):\n",
    "    def __init__(self, bi_encoder , csv_path, batch_size, num_epochs, num_warmup_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bi_encoder = bi_encoder\n",
    "        self.csv_path = csv_path\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = num_epochs\n",
    "        self.warmup_steps = num_warmup_steps\n",
    "\n",
    "        self.act_function = torch.nn.Sigmoid()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "        self.best_val_loss = 999 # Init with big value so that 1st epoch is always saved\n",
    "\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        train_data = df.iloc[:-config.validation_size]\n",
    "        val_data = df.iloc[-config.validation_size:]\n",
    "\n",
    "        self.train_dataset = MatchDataset(train_data)\n",
    "        self.val_dataset = MatchDataset(val_data)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=config.batch_size, shuffle=config.shuffle_train, collate_fn=self.bi_encoder.smart_batching_collate, num_workers=40)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=self.bi_encoder.smart_batching_collate, num_workers=40)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.bi_encoder(input)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        (addresses, target) = batch\n",
    "        addresses_embedding_unpack = [self.forward(address)['sentence_embedding'] for address in addresses]\n",
    "        address_1_embedding, address_2_embedding = addresses_embedding_unpack\n",
    "\n",
    "        scores = util.cos_sim(address_1_embedding, address_2_embedding)*20\n",
    "        target = torch.tensor(range(address_1_embedding.shape[0]), dtype=torch.long, device=self.bi_encoder.device) \n",
    "\n",
    "        loss = self.loss_function(scores, target)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        (addresses, target) = batch\n",
    "        addresses_embedding_unpack = [self.forward(address)['sentence_embedding'] for address in addresses]\n",
    "        address_1_embedding, address_2_embedding = addresses_embedding_unpack\n",
    "\n",
    "        scores = util.cos_sim(address_1_embedding, address_2_embedding)*20\n",
    "        target = torch.tensor(range(address_1_embedding.shape[0]), dtype=torch.long, device=self.bi_encoder.device) \n",
    "\n",
    "        loss = self.loss_function(scores, target)\n",
    "\n",
    "        return {'val_loss': loss} #Isto na realidade não é 1 valor, vão ser tantos valores quantos batches tiver o validation_step\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        # Obter as médias dos valores de métricas calculadas em todos os batches de validation.\n",
    "        current_epoch_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean() #calculate the average validation loss across all the batches in the validation set.   \n",
    "\n",
    "\n",
    "        # Logar as métricas\n",
    "        self.log('val_loss', current_epoch_val_loss, prog_bar=True, sync_dist=True)\n",
    "\n",
    "\n",
    "        # Verificar se quero dar save ao modelo. Comparo a loss obtida com a melhor loss do momento\n",
    "        print('\\nVerifico se quero savar o modelo: ')\n",
    "        print('Loss deste epoch = {}, e melhor loss do momento = {}'.format(current_epoch_val_loss, self.best_val_loss))\n",
    "        if current_epoch_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_epoch_val_loss\n",
    "            print('Savei o modelo')\n",
    "            self.bi_encoder.save(config.model_save_path)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer_config = {\n",
    "            'params' : self.bi_encoder.parameters(),\n",
    "            'lr' : config.learning_rate,\n",
    "            'weight_decay' : config.weight_decay,\n",
    "        }\n",
    "        \n",
    "        optimizer = AdamW(**optimizer_config)\n",
    "        total_steps=self.trainer.estimated_stepping_batches #Esta função tem em conta o valor de quantos batches são accumulated\n",
    "\n",
    "        if config.warmup_function == \"Linear\":\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= int(config.warmup_steps * total_steps), num_training_steps= total_steps)\n",
    "        else:\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=total_steps , eta_min = 5e-7)\n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first line creates an instance of the <code>LearningRateMonitor</code> class and assigns it to the variable <code>lr_monitor</code>. This callback monitors the learning rate of the optimizer and logs it at a specified interval during training.</p>\n",
    "<p>The second line creates an instance of the <code>EarlyStopping</code> class and assigns it to the variable <code>early_stopping_callback</code>. This callback monitors the validation loss and stops training early if the loss does not improve for a certain number of epochs specified by the <code>patience</code> argument. The <code>monitor</code> argument specifies which metric to monitor, in this case, the validation loss. The <code>mode</code> argument specifies whether to minimize or maximize the monitored metric, in this case, we want to minimize the validation loss.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=config.patience, mode='min')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code creates an instance of the `SentenceTransformer` class using a pre-trained transformer-based language model specified in `config.model_name`.<br>\n",
    "It first creates a `Transformer` model using `config.model_name` with a maximum sequence length of 128.<br>\n",
    "It then creates a `Pooling` model to obtain a fixed-size sentence embedding from the transformer output.<br>\n",
    "Finally, a fully connected `Dense` model with `nn.Tanh()` activation function is used to further transform the sentence embeddings. These three models are passed as a list to the `SentenceTransformer` class constructor, which creates the final model.<br>\n",
    "\n",
    "\n",
    "The code also sets the device for running the model to \"cuda\" if a GPU is available, otherwise \"cpu\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "word_embedding_model = models.Transformer(config.model_name, max_seq_length=128)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=512, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model], device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>The code initializes a <code>MatchModel</code> with the <code>SentenceTransformer</code> model created above and a <code>Trainer</code> with various arguments:</p>\n",
    "<ul>\n",
    "<li><code>accelerator</code> is set to 'gpu'</li>\n",
    "<li><code>devices</code> is set to 1</li>\n",
    "<li><code>callbacks</code> is set to a list containing <code>lr_monitor</code> and <code>early_stopping_callback</code> if <code>config_wandb.early_stop</code> is <code>True</code>, otherwise only <code>lr_monitor</code> is included</li>\n",
    "<li><code>check_val_every_n_epoch</code> is set to 1</li>\n",
    "<li><code>max_epochs</code> is set to <code>config_wandb.max_epochs</code></li>\n",
    "<li><code>enable_checkpointing</code> is set to <code>False</code></li>\n",
    "<li><code>accumulate_grad_batches</code> is set to 1</li>\n",
    "<li><code>logger</code> is set to <code>wandb_logger</code></li>\n",
    "<li><code>log_every_n_steps</code> is set to 1</li>\n",
    "<li><code>deterministic</code> is set to <code>True</code></li>\n",
    "<li><code>precision</code> is set to 16</li>\n",
    "</ul>\n",
    "<p>Finally, the <code>trainer</code> is used to fit the <code>model</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MatchModel(bi_encoder = model, \n",
    "                    csv_path = config.csv_path,\n",
    "                    batch_size = config.batch_size, \n",
    "                    num_epochs = config.max_epochs,\n",
    "                    num_warmup_steps = config.warmup_steps)\n",
    "\n",
    "trainer = Trainer(\n",
    "                    accelerator='gpu',\n",
    "                    devices=1,\n",
    "                    callbacks = [lr_monitor, early_stopping_callback] if config.early_stop == True else [lr_monitor],\n",
    "                    check_val_every_n_epoch=1,\n",
    "                    max_epochs=config.max_epochs,\n",
    "                    logger=wandb_logger,\n",
    "                    log_every_n_steps=1,\n",
    "                    enable_checkpointing=False,\n",
    "                    deterministic=True,\n",
    "                    precision = 16,\n",
    "                    )\n",
    "trainer.fit(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.alert(\n",
    "    title=f\"Finish {config.run_name}\", \n",
    "    text = f\"Run is over.\",\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
