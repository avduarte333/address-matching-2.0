{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_for_tokenizer.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_for_tokenizer.csv a file with a lot of possible unnormalized and normalized addresses in a column named '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        texto = self.data.iloc[idx]['0']\n",
    "        return {'train': texto}\n",
    "\n",
    "dataset_aux = NormalizedDataset(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = dataset_aux[:][\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece Tokenizer<br>\n",
    "https://huggingface.co/course/chapter6/8?fw=pt#acquiring-a-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above code sets the <code>normalizer</code> of a tokenizer object. The <code>normalizer</code> is a Sequence of operations that will be applied to the input text before tokenization. In this case, the <code>normalizer</code> is composed of three operations: </p>\n",
    "<ul>\n",
    "    <li><code>normalizers.NFD()</code> - decomposes the text into its constituent graphemes, separating any accent marks or diacritical characters from their base character</li>\n",
    "    <li><code>normalizers.Lowercase()</code> - converts all characters to lowercase</li>\n",
    "    <li><code>normalizers.StripAccents()</code> - removes any remaining accent marks or diacritical characters from the text</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code is setting up a pre-tokenizer for a ``tokenizer`` object. Specifically, it is setting up a sequence of pre-tokenizers, each of which will be applied to the input text before tokenization.\n",
    "\n",
    "The ``pre_tokenizers.Sequence()`` method is used to create a sequence of pre-tokenizers that will be applied in order. In this case, there are three pre-tokenizers in the sequence:\n",
    "<ul>\n",
    "    <li><code>pre_tokenizers.Whitespace()</code> - This pre-tokenizer will split the text on whitespace characters (spaces, tabs, and newlines).</li>\n",
    "    <li><code>pre_tokenizers.Digits(False)</code> - This pre-tokenizer will split off any sequences of digits (0-9) from the text. The False argument specifies that these digit sequences should not be returned as individual tokens.</li>\n",
    "    <li><code>pre_tokenizers.Punctuation('removed')</code> - This pre-tokenizer will remove any punctuation characters from the text.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.Whitespace(), pre_tokenizers.Digits(False), pre_tokenizers.Punctuation('removed')]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided code, a <code>WordPieceTrainer</code> object is created from the <code>trainers</code> module. This object is then configured with several parameters:\n",
    "\n",
    "<ul>\n",
    "    <li><code>special_tokens</code>: a list of special tokens to be included in the vocabulary of the trained tokenizer. The special tokens are <code>[UNK]</code>, <code>[PAD]</code>, <code>[CLS]</code>, <code>[SEP]</code>, and <code>[MASK]</code>.</li>\n",
    "    <li><code>show_progress</code>: a boolean value indicating whether or not to display progress bars during training.</li>\n",
    "    <li><code>min_frequency</code>: an integer value indicating the minimum frequency a subword must have to be included in the final vocabulary.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(special_tokens=special_tokens, show_progress=True, min_frequency=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The code is using the <code>train_from_iterator()</code> method of a tokenizer object to train the tokenizer from an iterator that provides training data. <br>\n",
    "The training is performed using a specified trainer object, which is passed to the <code>train_from_iterator()</code> method as an argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>The above code is performing the following tasks:</p>\n",
    "<ul>\n",
    "<li>Get the token ID for the \"[CLS]\" and \"[SEP]\" tokens using the <code>tokenizer.token_to_id()</code> function.</li>\n",
    "<li>Set up the <code>tokenizer.post_processor</code> using the <code>processors.TemplateProcessing()</code> function to add the special tokens and combine the input sequence(s) with the special tokens.</li>\n",
    "<li>The <code>single</code> and <code>pair</code> parameters specify the templates to be used when processing a single sequence or a pair of sequences respectively.</li>\n",
    "<li>The <code>special_tokens</code> parameter specifies the special tokens and their corresponding token IDs to be added to the input sequence(s).</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The code is doing the following:</p>\n",
    "<ul>\n",
    "  <li>Setting the <code>WordPiece</code> decoder for the <code>tokenizer</code> object</li>\n",
    "  <li>Saving the <code>tokenizer</code> object in <code>JSON</code> format</li>\n",
    "  <li>Creating a new instance of <code>BertTokenizerFast</code> from the saved tokenizer object and assigning it to a variable <code>wrapped_tokenizer</code></li>\n",
    "  <li>Saving the <code>wrapped_tokenizer</code> in a folder named 'NewCustomTokenizer'</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "from transformers import BertTokenizerFast\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.save_pretrained('Custom_Tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
